# -*- coding: utf-8 -*-
"""jbastuba_assignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Z2_4I1IsRFSiVqwi6g24tHgPDQA0_UN

# Time Series Forecasting with RNNs

We will use ETTh1 dataset which records the oil temperature for a transformer every hour for two years. 
Our goal here is to predict oil temperature 96 hours into the future.
"""

import os
import datetime
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from tensorflow.keras.optimizers import Adam
from statsmodels.tsa.seasonal import seasonal_decompose

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

cd /content/drive/MyDrive

df = pd.read_csv('ETTh1.csv')

"""## 1. Inspecting data"""

df.head(10)

df.describe()

df.info()

plt.plot(df['OT'], label='OT')
plt.title('OT')
plt.show()

"""The amplitude of the cycles remains relatively constant, thus this feature may be additive."""

#Take a peek at the first 90 days of data just for another means of inspection
ot = df['OT']
plt.xticks(np.arange(0, 90*24, 24), rotation=45)
plt.plot(range(90*24), ot[:90*24])

"""There does not appear to be an obvious trend but the daily flucuations do appear to be consistent and in 
this period of time there was a general increase, followed by a general decrease in oil temperature."""

result = seasonal_decompose(df['OT'], model='additive', period=1)
result.plot()
plt.show()

"""Per the seasonal decomposition using statsmodel, the entire series was taken as the trend component 
and there was no seasonality.

## 2. Feature Engineering

There was no periodicity per the previous inspection, so the feature 'date' will be removed.
"""

df = df.sort_values(by='date')
df = df.drop(["date"], axis=1)

df.head(5)

#Making the time series STATIONARY (step 6)
df['OT'] = df['OT'].diff().fillna(0)

df.head(5)

#drop the first row where differencing cannot be calculated
df = df.drop(index=0)
df.head(5)

"""## 3. Split, Normalize, & Create Time Series Data

### Split the data
"""

n = len(df)

#Use the first 12 months (12*30*24) for training
num_train_samples = 12 * 30 * 24

#Use the next 6 months (6*30*24) for validation
num_val_samples = 6 * 30 * 24

#Use the remainder for test
num_test_samples = n - num_train_samples - num_val_samples

print("number of train samples", num_train_samples)
print("validation shape", num_val_samples)
print("test shape", num_test_samples)

"""### Normalize"""

#normalize the data based on the training dataset's mean and standard deviation
train_mean = df[:num_train_samples].mean(axis=0)
train_std = df[:num_train_samples].std(axis=0)
df = (df - train_mean) / train_std

df.head()

"""### Create time series data"""

#sequence_length=5 days prior (120 hours)
sequence_length = 48
batch_size = 256
#We want to forecast 96 hours into the future
horizon= 96

# delay is where the index for y should start
delay=sequence_length+horizon-1

train_dataset = keras.preprocessing.timeseries_dataset_from_array(
  data=df[:-delay], #remove the last observations from the dataframe that will not have a time series predictor
  targets=df['OT'][delay:],
  sequence_length=sequence_length,
  batch_size=batch_size,
  start_index=0,
  end_index=num_train_samples)


val_dataset = keras.preprocessing.timeseries_dataset_from_array(
  data=df[:-delay], #remove the last observations from the dataframe that will not have a time series predictor
  targets=df['OT'][delay:],
  sequence_length=sequence_length,
  batch_size=batch_size,
  start_index=num_train_samples,
  end_index=num_train_samples + num_val_samples)


test_dataset = keras.preprocessing.timeseries_dataset_from_array(
  data=df[:-delay].to_numpy(), #remove the last observations from the dataframe that will not have a time series predictor
  targets=df['OT'][delay:],
  sequence_length=sequence_length,
  batch_size=batch_size,
  start_index=num_train_samples + num_val_samples)

"""## 4. A Commonsense Baseline Model"""

#un-normalize the predictions and targets, to convert them back to celsius so we can interpret the error.
#To un_normalize, reverse the normalization process, that is, multiply by train_std and add train_mean
def unnormalize(preds, targets, mean, std):
   preds=preds* std + mean
   targets= targets*std+mean
   return preds, targets

def evaluate_model(dataset, model=None):
  total_abs_err = 0.
  num_sequences = 0
  # the index of the oil temperature (OT) variable in the data. OT is the seventh column in df so its index is 6.
  temp_index=6
  for samples, targets in dataset:

    #if model is None, use commonsense baseline, that is predict the target to be its last measurement in the input sequence
    if model==None:
      #for each sequence in the batch predict the target to be samples[:, -1, temp_index] which is 
      #the last temperature measurement in the input sequence
      #index -1 in samples[:, -1, temp_index] means the last time step in the sequence
      preds = samples[:, -1, temp_index]

    #if model is not None, get its predictions
    else:
      preds= model.predict(samples, verbose=0).flatten()

    preds, targets=unnormalize(preds, targets, train_mean.iloc[temp_index], train_std.iloc[temp_index])

    # add the absolute difference between the predictions and targets to total_abs_err
    total_abs_err += np.sum(np.abs(preds - targets))

    # add the number of sequences in this batch to num_sequences
    num_sequences += samples.shape[0]

  #get the average absolute error
  return total_abs_err / num_sequences


print(f"Validation MAE Baseline: {evaluate_model(val_dataset):.2f}")

"""The baseline predictions have a MAE of 2.63. So if you always assume that the temperature 48 hours from now 
will be the same as it is right now, you will be incorrect by 2.63 degrees.

After differencing: Baseline MAE decreased to 0.74

## 5. LSTM and 1d Convolution Models
"""

#every sample in train/validation/test is a 2d tensor of shape (sequence_length, num_features)
num_features=df.shape[-1]
def build_lstm(sequence_length, num_features):
  inputs = keras.Input(shape=(sequence_length, num_features))
  x = layers.LSTM(16)(inputs)
  outputs = layers.Dense(1)(x)
  lstm_model = keras.Model(inputs, outputs)
  return lstm_model

lstm_model= build_lstm(sequence_length, num_features)

# check point the model with lowest validation loss
checkpoint= keras.callbacks.ModelCheckpoint("temp_lstm.keras",save_best_only=True)

#add a learning rate schedule to reduce the learning rate when validation loss plateaus
rop= keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)

#stop trianing if validation loss does not imrove for 5 consecutive epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-4, restore_best_weights=True)


lstm_model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = lstm_model.fit(train_dataset,
          epochs=50,
          validation_data=val_dataset,
          callbacks=[checkpoint, early_stopping, rop])

print(lstm_model.summary())

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss recurrent baseline')
plt.legend()
plt.show()

# load the checkpointed model and compute its test MAE
lstm_model = keras.models.load_model("temp_lstm.keras")

print(f"Validation MAE lstm: {evaluate_model(val_dataset, model=lstm_model):.2f}")

"""The baseline LSTM resulted in an MAE of 4.39, which is two times higher than the commonsense baseline model. 
So far this baseline LSTM is not performing better and is overfitting. Perhaps after some tuning I can get it to 
perform at least as well as the commonsense baseline model.

The stationary time series LSTM has an MAE of 0.53. This is an improvement to the commonsense baseline of 0.74.

### Tuning to improve performance

I will stack GRU layers, add a dense+dropout layer after the GRU layers but before the final layer.
"""

num_features=df.shape[-1]
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-4, restore_best_weights=True)

def build_stacked_lstm(sequence_length, num_features):
  inputs = keras.Input(shape=(sequence_length, num_features))
  x = layers.GRU(32, return_sequences=True)(inputs)
  x = layers.GRU(32)(x)
  x = layers.Dropout(0.5)(x)
  x = layers.Dense(16, activation="relu")(x)
  x = layers.Dropout(0.5)(x)
  outputs = layers.Dense(1)(x)
  model = keras.Model(inputs, outputs)
  return model

stacked_lstm_model= build_stacked_lstm(sequence_length, num_features)

# check point the best model
checkpoint= keras.callbacks.ModelCheckpoint("ot_stacked_lstm.keras", save_best_only=True)

stacked_lstm_model.compile(optimizer=keras.optimizers.AdamW(learning_rate=0.001,weight_decay=0.002), loss="mse", metrics=["mae"])
history = stacked_lstm_model.fit(train_dataset,
          epochs=50,
          validation_data=val_dataset,
          callbacks=[checkpoint, early_stopping, rop])

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

# load the checkpointed model and compute its test MAE
stacked_lstm_model = keras.models.load_model("ot_stacked_lstm.keras")

print(f"Validation MAE Stacked lstm w/ GRU layers: {evaluate_model(val_dataset, model=stacked_lstm_model):.2f}")

"""**Increasing the capacity: stacked GRU layer + dense + dropout layer**

The MAE on the validation data has decreased ever so slightly from 4.57 to 4.48 with the additional capacity. 
The model has improved very well as far as *not* overfitting now.

**Tune the sequence_length parameter in the time series creation**

sequence_length = 48 --> MAE: 4.27

sequence_length = 96 --> MAE: 4.81

sequence_length = 120 --> MAE: 4.48

sequence_length = 336 --> MAE: 4.47

**Using AdamW and adjusting initial learning rate**

Adding AdamW and specifying the initial learning rate did lead to a significant improvement in the validation MAE (3.87). 
However the graph of the losses is wild, with a lot of flucuation and very early stopping.



**Selected hyperparameters**

Using the following hyperparameters, I was able to get the MAE down to 3.76.

Initial learning rate: 0.005

Learning rate decay rate: 0.2

LR decay Patience: 3

sequence_length: 48

Adam weight decay: 0.002

epochs: 50

Neurons: 32 GRU, 32 GRU, Dropout (0.5), 16 Dense, Dropout (0.5)

## 1d convolution model
"""

def build_1dconv(sequence_length, num_features):
  inputs = keras.Input(shape=(sequence_length, num_features))  #shape (48, 7)

  # a 1D convolutional layer with 8 filters and a kernel/window of size 12
  x = layers.Conv1D(8, 12, activation="relu")(inputs)
  x = layers.MaxPooling1D(2, padding='same')(x)

  # a 1D convolutional layer with 8 filters and a kernel of size 4
  x = layers.Conv1D(8, 7, activation="relu")(x)
  x = layers.MaxPooling1D(2, padding='same')(x)

  # a 1D convolutional layer with 8 filters and a kernel of size 4
  x = layers.Conv1D(8, 4, activation="relu")(x)

  # a global average pooling layer that takes the global average along the temporal dimension
  x = layers.GlobalAveragePooling1D()(x)
  outputs = layers.Dense(1)(x)
  model = keras.Model(inputs, outputs)
  return model

conv1D_model= build_1dconv(sequence_length, num_features)

conv1D_model.summary()

# check point the best model
checkpoint= keras.callbacks.ModelCheckpoint("ot_conv1d.keras",save_best_only=True)
conv1D_model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = conv1D_model.fit(train_dataset,
epochs=50,
validation_data=val_dataset,
callbacks=[checkpoint, rop])

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

# load the checkpointed model and compute its test MAE
conv1D_model = keras.models.load_model("ot_conv1d.keras")

print(f"Validation MAE ConvNet 1D: {evaluate_model(val_dataset, model=conv1D_model):.2f}")

"""Time series:

The 1D conv net was able to achieve an MAE of 4.33. The stacked GRU model performed the best thus far with an MAE of 3.76.

Stationary time series:

The 1D conv net had an average error (MAE) of 0.54.

## 6. Feature Engineering - Making the Time Series Stationary

After implementing differencing (making the time series stationary), the MAE values are:

Baseline: 0.74

LSTM model: 0.53

Stacked GRU: 0.52

1D Convolutional network: 0.54

This indicates that the average error between prediction and actual value is around 0.5 for the neural networks, 
which is a tremendous improvement from the non-stationary time series error rate. The neural network models outperform 
the commonsense baseline model and thus prove their value in this instance.

## 7. Performance on the test set

The stacked LSTM with GRU layers performed the best on the validation data (MAE of 0.52), thus I will use the 
checkpointed stacked_lstm model on the test dataset to gather its predictions.
"""

# load the checkpointed GRU model and compute its test MAE
best_model = keras.models.load_model("ot_stacked_lstm.keras")

print(f"test MAE for best model: {evaluate_model(test_dataset, model = best_model):.2f}")
print(f"test MAE for commonsense model: {evaluate_model(test_dataset):.2f}")

preds = best_model.predict(test_dataset, verbose=0).flatten()

#the following line was slightly adapted from 
https://stackoverflow.com/questions/56226621/how-to-extract-data-labels-back-from-tensorflow-dataset
targets = np.concatenate([targets for inputs, targets in test_dataset], axis=0)
print(preds.shape)
print(targets.shape)

#plot the predicted vs true values for the test set
fig, (ax1, ax2) = plt.subplots(1, 2)  # 1 row, 2 columns
fig.suptitle('Predicted vs True')
ax1.plot(preds, 'ro', label='Predicted')
ax1.set_title('Predicted')
ax2.plot(targets, 'bo', label='True')
ax2.set_title('True')
plt.show()

"""The graph certainly demonstrates the level of error that the model routinely predicts. The predicted values are less 
dense around 0 compared to the true values and are more spread out along the y axis. Regardless, the overall mean absolute 
error of 0.44 is pretty good and beats the baseline model (0.62)."""
